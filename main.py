import os
import shutil

# --- è¨­å®šé …ç›® (å¤‰æ›´) ---
# æ¤œç´¢å¯¾è±¡ã®ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆãŒæ ¼ç´ã•ã‚Œã¦ã„ã‚‹ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®ãƒªã‚¹ãƒˆ
DATA_PATHS = ["wiki/", "diff/"]
# æ¤œç´¢ã™ã‚‹ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’.txtã«é™å®š
GLOB_PATTERN = "**/*.txt"
# ãƒ™ã‚¯ãƒˆãƒ«ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã®ä¿å­˜å…ˆ
DB_PATH = "vector_db/"
# ãƒ­ãƒ¼ã‚«ãƒ«ã®åŸ‹ã‚è¾¼ã¿ãƒ¢ãƒ‡ãƒ«ã®ãƒ‘ã‚¹
EMBED_MODEL_PATH = "./glucose"
# Ollamaã§ç™»éŒ²ã—ãŸãƒ­ãƒ¼ã‚«ãƒ«LLMã®åå‰
LLM_MODEL_NAME = "elyza-local"


def main():
    """
    RAGã‚·ã‚¹ãƒ†ãƒ ã®ãƒ¡ã‚¤ãƒ³å‡¦ç†
    """
    from langchain_community.document_loaders import DirectoryLoader
    from langchain.text_splitter import RecursiveCharacterTextSplitter
    from langchain_community.vectorstores import Chroma
    from langchain_huggingface import HuggingFaceEmbeddings  # ä¿®æ­£
    from langchain_community.llms import Ollama
    from langchain.chains import RetrievalQA
    from langchain.prompts import PromptTemplate

    # 1. åŸ‹ã‚è¾¼ã¿ãƒ¢ãƒ‡ãƒ«ã®æº–å‚™
    print("åŸ‹ã‚è¾¼ã¿ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰ä¸­...")
    embeddings = HuggingFaceEmbeddings(model_name=EMBED_MODEL_PATH)
    print("ãƒ­ãƒ¼ãƒ‰å®Œäº†ã€‚")

    # 2. ãƒ™ã‚¯ãƒˆãƒ«ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã®æº–å‚™
    if not os.path.exists(DB_PATH):
        print(f"ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ãŒå­˜åœ¨ã—ãªã„ãŸã‚ã€æ–°è¦ä½œæˆã—ã¾ã™ã€‚")
        
        # --- ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã®èª­ã¿è¾¼ã¿ (å¤‰æ›´) ---
        all_documents = []
        print(f"å¯¾è±¡ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª: {DATA_PATHS}")
        for path in DATA_PATHS:
            if not os.path.isdir(path):
                print(f"è­¦å‘Š: ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª '{path}' ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™ã€‚")
                continue
            
            print(f"'{path}' ã‹ã‚‰.txtãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã¿ä¸­...")
            from langchain_community.document_loaders import TextLoader
            loader = DirectoryLoader(
                path,
                glob=GLOB_PATTERN,
                recursive=True,
                show_progress=True,
                use_multithreading=True,
                loader_cls=TextLoader,
                loader_kwargs={'encoding': 'utf-8'}
            )
            documents = loader.load()
            all_documents.extend(documents)

        if not all_documents:
            print("ã‚¨ãƒ©ãƒ¼: å¯¾è±¡ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã«.txtãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚å‡¦ç†ã‚’ä¸­æ–­ã—ã¾ã™ã€‚")
            return

        # ãƒ†ã‚­ã‚¹ãƒˆã®åˆ†å‰²
        text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)
        texts = text_splitter.split_documents(all_documents)
        
        # ãƒ™ã‚¯ãƒˆãƒ«åŒ–ã—ã¦Chromaã«ä¿å­˜
        print("ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‚’æ§‹ç¯‰ä¸­...ï¼ˆæ™‚é–“ãŒã‹ã‹ã‚‹å ´åˆãŒã‚ã‚Šã¾ã™ï¼‰")
        db = Chroma.from_documents(texts, embeddings, persist_directory=DB_PATH)
        db.persist()
        print("ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã®æ§‹ç¯‰ãŒå®Œäº†ã—ã¾ã—ãŸã€‚")
    else:
        print(f"æ—¢å­˜ã®ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‚’'{DB_PATH}'ã‹ã‚‰ãƒ­ãƒ¼ãƒ‰ã—ã¾ã™ã€‚")
    
    db = Chroma(persist_directory=DB_PATH, embedding_function=embeddings)

    # 3. LLMã¨QAãƒã‚§ãƒ¼ãƒ³ã®æº–å‚™
    llm = Ollama(model=LLM_MODEL_NAME,temperature=0.0)
    
    prompt_template = """
    ã‚ãªãŸã¯ã€æä¾›ã•ã‚ŒãŸã€ŒKMCã®å†…éƒ¨wikiã¨ã„ã†ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆæƒ…å ±ã€ã ã‘ã‚’çŸ¥è­˜æºã¨ã™ã‚‹ã€éå¸¸ã«å³æ ¼ã§å¿ å®Ÿãªã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚
    ã‚ãªãŸã®å”¯ä¸€ã®ã‚¿ã‚¹ã‚¯ã¯ã€ä»¥ä¸‹ã®ãƒ«ãƒ¼ãƒ«ã«å¾“ã£ã¦å†…éƒ¨wikiã«æ›¸ã‹ã‚ŒãŸå†…å®¹ã«é–¢ã™ã‚‹ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è³ªå•ã«å›ç­”ã™ã‚‹ã“ã¨ã§ã™ã€‚

    # ãƒ«ãƒ¼ãƒ«
    - å›ç­”ã¯ã€å¿…ãšã€Œã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆæƒ…å ±ã€ã«æ›¸ã‹ã‚Œã¦ã„ã‚‹å†…å®¹ã®ã¿ã‚’æ ¹æ‹ ã¨ã—ãªã‘ã‚Œã°ãªã‚Šã¾ã›ã‚“ã€‚
    - ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆæƒ…å ±ã«ç­”ãˆãŒè¨˜è¼‰ã•ã‚Œã¦ã„ãªã„ã€ã¾ãŸã¯é–¢é€£æ€§ãŒä½ã„å ´åˆã¯ã€ä»–ã®çŸ¥è­˜ã‚„æ¨æ¸¬ã‚’ä¸€åˆ‡å«ã‚ãªã„ã§ãã ã•ã„ã€‚
    - è‡ªèº«ã®æ„è¦‹ã‚„ã€ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆå¤–ã®æƒ…å ±ã‚’ä»˜ã‘åŠ ãˆã¦ã¯ã„ã‘ã¾ã›ã‚“ã€‚

    ---
    ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆæƒ…å ±:
    {context}
    ---
    è³ªå•: {question}
    ---
    ä¸Šè¨˜ã®ãƒ«ãƒ¼ãƒ«ã«å³æ ¼ã«å¾“ã£ãŸå›ç­”:
    """
    PROMPT = PromptTemplate(
        template=prompt_template, input_variables=["context", "question"]
    )
    
    qa_chain = RetrievalQA.from_chain_type(
        llm=llm,
        chain_type="stuff",
        retriever=db.as_retriever(),
        chain_type_kwargs={"prompt": PROMPT},
        return_source_documents=True
    )

    # 4. è³ªå•å¿œç­”ãƒ«ãƒ¼ãƒ—
    print("\n--- è³ªå•å¿œç­”ã‚’é–‹å§‹ã—ã¾ã™ --- (çµ‚äº†ã™ã‚‹ã«ã¯ 'exit' ã¨å…¥åŠ›)")
    while True:
        query = input("\nè³ªå•ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„: ")
        if query.lower() == 'exit':
            break
        if not query.strip():
            continue

        print("\nå›ç­”ã‚’ç”Ÿæˆä¸­...")
        result = qa_chain.invoke(query)
        
        print("\nâœ… å›ç­”:")
        print(result["result"])
        
        print("\nğŸ“š å‚ç…§ã—ãŸæƒ…å ±æº:")
        sources = {doc.metadata.get("source", "N/A") for doc in result["source_documents"]}
        for src in sources:
            print(f"- {src}")


if __name__ == "__main__":
    main()